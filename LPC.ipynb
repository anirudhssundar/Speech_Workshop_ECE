{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Prediction Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Linear Predictive Coding(LPC) provides an accurate and economical representation of relevant speech parameters that can reduce transmission rates in speech coding, increase accuracy and reduce calculation in speech recognition, and generate efficient speech synthesis. \n",
    "\n",
    "The basic idea behind LPC is that a speech sample can be approximated as a linear combination of past speech samples. By minimizing the sum of squared differences over a finite interval between actual speech samples and the linearly predicted ones, a unique set of predictor coefficients can be determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We first need to import the required Python libraries for the entire anaylsis  \n",
    "Prerequisites required in this implementation are:\n",
    "```\n",
    "Python 3.x \n",
    "numpy   \n",
    "scipy\n",
    "librosa\n",
    "```\n",
    "\n",
    "You can invoke any required library into your workspace as:  \n",
    "`import package_name`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as sig\n",
    "import scipy.io.wavfile as wav\n",
    "import scipy.linalg as linalg\n",
    "import librosa\n",
    "from scipy.linalg import solve_toeplitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the file name, and setting the order of the LPC model as 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"audio_files/spectrogram_audio.wav\"\n",
    "sample_rate, signal = wav.read(filename)\n",
    "order = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Applying pre-emphasis and windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_emphasis = 0.97\n",
    "emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
    "frame_size = 0.025 \n",
    "frame_stride = 0.01\n",
    "frame_length, frame_step = frame_size * sample_rate, frame_stride * sample_rate  # Convert from seconds to samples\n",
    "signal_length = len(emphasized_signal)\n",
    "frame_length = int(round(frame_length))\n",
    "frame_step = int(round(frame_step))\n",
    "num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))  # Make sure that we have at least 1 frame\n",
    "pad_signal_length = num_frames * frame_step + frame_length\n",
    "z = np.zeros((pad_signal_length - signal_length))\n",
    "pad_signal = np.append(emphasized_signal, z) # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal\n",
    "indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "frames *= np.blackman(frame_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing LPC for 1 frame\n",
    "Using Autocorrelation method to generate a toeplitz matrix and solving it using Levsion-Durbin Recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.21964622 -0.05056925  0.03951827 -0.04997397 -0.19916233 -0.05241685\n",
      "  0.00071193 -0.16961096 -0.01992969  0.0740895  -0.04449198 -0.07419442\n",
      "  0.08100477]\n"
     ]
    }
   ],
   "source": [
    "autocorrelation = sig.fftconvolve(frames[0], frames[0][::-1])\n",
    "autocorr_coefficients = autocorrelation[int(len(autocorrelation)/2):][:(order + 1)]\n",
    "lpc_coefficients = -1*solve_toeplitz(autocorr_coefficients[:order],autocorr_coefficients[1:order+1])\n",
    "print(lpc_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using librosa library to compute the same LPC coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.21964607 -0.05056932  0.03951846 -0.0499739  -0.19916262 -0.05241687\n",
      "  0.00071183 -0.16961147 -0.01992988  0.07408958 -0.04449215 -0.07419442\n",
      "  0.08100641]\n"
     ]
    }
   ],
   "source": [
    "lpc_librosa = librosa.lpc(frames[0],order=13)[1:]\n",
    "print(lpc_librosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making this whole flow into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lpc_coeff(filename, order=13, write_file = False):\n",
    "    sample_rate, signal = wav.read(filename)\n",
    "    pre_emphasis = 0.97\n",
    "    emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
    "    frame_size = 0.025 \n",
    "    frame_stride = 0.01\n",
    "    frame_length, frame_step = frame_size * sample_rate, frame_stride * sample_rate  # Convert from seconds to samples\n",
    "    signal_length = len(emphasized_signal)\n",
    "    frame_length = int(round(frame_length))\n",
    "    frame_step = int(round(frame_step))\n",
    "    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))  # Make sure that we have at least 1 frame\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = np.zeros((pad_signal_length - signal_length))\n",
    "    pad_signal = np.append(emphasized_signal, z) # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal\n",
    "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "    \n",
    "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "    frames *= np.blackman(frame_length)\n",
    "    lpc_coefficients = np.zeros((num_frames,order))\n",
    "    lpc_librosa = np.zeros((num_frames,order))\n",
    "    \n",
    "    for i in range(0,len(frames),1):\n",
    "        autocorrelation = sig.fftconvolve(frames[i], frames[i][::-1])\n",
    "        autocorr_coefficients = autocorrelation[int(len(autocorrelation)/2):]\n",
    "        lpc_coefficients[i] = solve_toeplitz(autocorr_coefficients[:order],autocorr_coefficients[1:order+1])\n",
    "        lpc_coefficients[i]*= -1\n",
    "        \n",
    "        lpc_librosa[i] = librosa.lpc(frames[i],order)[1:]\n",
    "        \n",
    "    error = np.sum(np.abs(lpc_librosa - lpc_coefficients))\n",
    "    print(\"Error per frame = \", error/num_frames)\n",
    "    \n",
    "    if write_file:\n",
    "        np.savetxt(\"LPC_coeff.txt\",lpc_coefficients)\n",
    "        \n",
    "    return lpc_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error per frame =  3.1781595209888165e-05\n"
     ]
    }
   ],
   "source": [
    "lpc_coeff = compute_lpc_coeff(\"audio_files/spectrogram_audio.wav\", 13, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
