{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel Frequency Cepstral Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "The first step in any speech recognition task is to extract features from the speech data  \n",
    "Mel-Frequecy Cepstral Coefficients is one such example of a feature that is highly robust and used widely for various speech processing tasks  \n",
    "\n",
    "MFCCs were first introduced in a 1980 paper by [Davis and Mermelstein](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.462.5073&rep=rep1&type=pdf)\n",
    "\n",
    "We will now move on to implementing MFCCs from first principles so as to better understand how it can be used for various speech recognition and processing tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Setup\n",
    "We first need to import the required Python libraries for the entire anaylsis  \n",
    "Prerequisites required in this implementation are:\n",
    "```\n",
    "Python 3.x \n",
    "numpy   \n",
    "scipy\n",
    "matplotlib\n",
    "```\n",
    "\n",
    "You can invoke any required library into your workspace as:  \n",
    "`import package_name`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "from scipy.fftpack import dct\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "audio_path = 'audio_files/free-spoken-digit-dataset-master/recordings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test audio signal\n",
    "sample_rate, signal = wav.read(os.path.join(audio_path,'5_jackson_1.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the signal\n",
    "plt.plot(signal)\n",
    "plt.xlabel('Sample number')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Sample Audio Signal Representation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Preprocessing\n",
    "\n",
    "### 1.1 Pre-emphasis\n",
    "Speech signals are generally band-limited to lower frequencies  \n",
    "However, some higher frequencies are often distorted due to higher noise in these regions  \n",
    "This helps improve the Signal-to-Noise Ratio (SNR) at higher frequencies  \n",
    "\n",
    "Pre-emphasis is generally described by the following equation:  \n",
    "`y(t) = x(t) - alpha*x(t)`  \n",
    "Here alpha is the pre-emphasis coefficient  \n",
    "Typical pre-emphasis coefficients are 0.95 or 0.97. Here we use `alpha=0.97`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-emphasis to ephasize higher frequencies\n",
    "pre_emphasis = 0.97\n",
    "emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Framing\n",
    "\n",
    "Performing a Fourier Transform on the signal directly will result in a loss of time-domain information  \n",
    "Thus, to retain information in both domains we assume that frequency content is stationary for a short period of time (a window) and perform the Fourier Transform on these windows\n",
    "\n",
    "Typical sizes for frames in speech processing is 20-40ms  \n",
    "Typical length of frame overlap is 10-15ms  \n",
    "\n",
    "We define `frame_size = 25ms` and `frame_stride = 10ms`. This represents an overlap of 15ms between frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define framing constants\n",
    "frame_size = 0.025 \n",
    "frame_stride = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Framing\n",
    "\n",
    "# Convert from seconds to samples\n",
    "frame_length = frame_size*sample_rate \n",
    "frame_step = frame_stride*sample_rate  \n",
    "\n",
    "signal_length = len(emphasized_signal)\n",
    "frame_length = int(round(frame_length))\n",
    "frame_step = int(round(frame_step))\n",
    "\n",
    "print ('frame length : ', frame_length, 'samples')\n",
    "print('frame stride : ', frame_step, 'samples')\n",
    "\n",
    "# Make sure that we have at least 1 frame\n",
    "num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))  \n",
    "\n",
    "pad_signal_length = num_frames * frame_step + frame_length\n",
    "z = np.zeros((pad_signal_length - signal_length))\n",
    "# Pad Signal to make sure that all frames have equal number of samples without truncating any samples \n",
    "# from the original signal\n",
    "pad_signal = np.append(emphasized_signal, z) \n",
    "\n",
    "# Generate indices of elements in each window\n",
    "indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "\n",
    "# Extract frames using indices\n",
    "frames = pad_signal[indices.astype(np.int32, copy=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Windowing\n",
    "\n",
    "A hamming window is applied to each frame to reduce the spectral leakage  \n",
    "[Click Here](https://dspillustrations.com/pages/posts/misc/spectral-leakage-zero-padding-and-frequency-resolution.html) to better understand spectral leakage and its effects on calculation of a DFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames *= np.hamming(frame_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Fast Fourier Transform\n",
    "\n",
    "We now calculate a 512-point FFT of each of the frames of the input audio signal  \n",
    "Using the resultant data we get an idea of the constituent frequencies of each windowed instant, giving more information on the phonemes present at that instant, which can be used to identify speech syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT\n",
    "NFFT = 512  # 512 point FFT\n",
    "mag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # Magnitude of the FFT\n",
    "pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))  # Power Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Filter Banks\n",
    "\n",
    "Now a set of triangular filter banks `n_filt = 40` is applied to the power spectrum to convert it into a Mel-Scale  \n",
    "The Mel-Scale is used to mimic the non-linear nature of human hearing - lower frequencies are picked up better than higher ones  \n",
    "The conversion between Hz and Mel is given as:  \n",
    "`m = 2595*log(1+ f/700)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter banks\n",
    "nfilt = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_freq_mel = 0\n",
    "high_freq_mel = (2595 * np.log10(1 + (sample_rate / 2) / 700))  # Convert Hz to Mel\n",
    "mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale\n",
    "hz_points = (700 * (10**(mel_points / 2595) - 1))  # Convert Mel to Hz\n",
    "bin = np.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "\n",
    "fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "for m in range(1, nfilt + 1):\n",
    "    f_m_minus = int(bin[m - 1])   # left\n",
    "    f_m = int(bin[m])             # center\n",
    "    f_m_plus = int(bin[m + 1])    # right\n",
    "    \n",
    "    # Create triangular portion\n",
    "    for k in range(f_m_minus, f_m):\n",
    "        fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "    for k in range(f_m, f_m_plus):\n",
    "        fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "\n",
    "# Apply Mel filter-banks to the power spectrum\n",
    "filter_banks = np.dot(pow_frames, fbank.T)\n",
    "filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)  # Numerical Stability\n",
    "filter_banks = 20 * np.log10(filter_banks)  # dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hz_points, mel_points)\n",
    "plt.xlabel('Hz')\n",
    "plt.ylabel('Mel')\n",
    "plt.title('Hertz to Mel conversion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident from the above figure that there is greater resolution in the lower frequencies in the Mel-Scale and higher frequencies appear to be closer together  \n",
    "Let us visualize the Mel-filterbanks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "for i in range(nfilt):\n",
    "    plt.plot(np.linspace(0, 3500, 257), fbank[i])\n",
    "\n",
    "plt.xlabel('Frequency in Hz')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Mel Filterbanks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Cepstral Coefficients\n",
    "\n",
    "We now calculate Mel-Frequency Cepstral Coefficients (MFCCs) on the Filter-Banks  \n",
    "As is evident from the filterbanks there is a high correlation between adjacent banks  \n",
    "We therefore apply a [Discrete Cosine Transform](http://www.svcl.ucsd.edu/courses/ece161c/handouts/DCT.pdf) to the filterbanks for decorrelation. From the resulting Cepstral Coefficients, we use coefficients 2-13 as our MFCC features `num_ceps = 12`  \n",
    "Higher cepstral coefficients are discarded as they represent very fast changes in speech. We can achieve similar results by considering only coefficients 2-13 as compared to all cepstral coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCC\n",
    "num_ceps = 12\n",
    "mfcc = dct(filter_banks, type=2, axis=1, norm='ortho')[:, 1 : (num_ceps + 1)] # Keep 2-13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Normalization (Optional)\n",
    "\n",
    "To balance the power spectrum we cna additionally subtract the mean from each of the frames  \n",
    "This has been shown to improve Signal-to-Noise Ratio (SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_banks -= (np.mean(filter_banks, axis=0) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc -= (np.mean(mfcc, axis=0) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc(filename):\n",
    "    sample_rate, signal = scipy.io.wavfile.read(filename)\n",
    "    pre_emphasis = 0.97\n",
    "    emphasized_signal = numpy.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
    "    frame_size = 0.025 \n",
    "    frame_stride = 0.01\n",
    "    frame_length, frame_step = frame_size * sample_rate, frame_stride * sample_rate  # Convert from seconds to samples\n",
    "    signal_length = len(emphasized_signal)\n",
    "    frame_length = int(round(frame_length))\n",
    "    frame_step = int(round(frame_step))\n",
    "    num_frames = int(numpy.ceil(float(numpy.abs(signal_length - frame_length)) / frame_step))  # Make sure that we have at least 1 frame\n",
    "\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = numpy.zeros((pad_signal_length - signal_length))\n",
    "    pad_signal = numpy.append(emphasized_signal, z) # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal\n",
    "\n",
    "    indices = numpy.tile(numpy.arange(0, frame_length), (num_frames, 1)) + numpy.tile(numpy.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "    frames = pad_signal[indices.astype(numpy.int32, copy=False)]\n",
    "    \n",
    "    frames *= numpy.hamming(frame_length)\n",
    "    # FFT\n",
    "    NFFT = 512  # 512 point FFT\n",
    "    mag_frames = numpy.absolute(numpy.fft.rfft(frames, NFFT))  # Magnitude of the FFT\n",
    "    pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))  # Power Spectrum\n",
    "    # Filter banks\n",
    "    nfilt = 40\n",
    "    low_freq_mel = 0\n",
    "    high_freq_mel = (2595 * numpy.log10(1 + (sample_rate / 2) / 700))  # Convert Hz to Mel\n",
    "    mel_points = numpy.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale\n",
    "    hz_points = (700 * (10**(mel_points / 2595) - 1))  # Convert Mel to Hz\n",
    "    bin = numpy.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "\n",
    "    fbank = numpy.zeros((nfilt, int(numpy.floor(NFFT / 2 + 1))))\n",
    "    for m in range(1, nfilt + 1):\n",
    "        f_m_minus = int(bin[m - 1])   # left\n",
    "        f_m = int(bin[m])             # center\n",
    "        f_m_plus = int(bin[m + 1])    # right\n",
    "\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    filter_banks = numpy.dot(pow_frames, fbank.T)\n",
    "    filter_banks = numpy.where(filter_banks == 0, numpy.finfo(float).eps, filter_banks)  # Numerical Stability\n",
    "    filter_banks = 20 * numpy.log10(filter_banks)  # dB\n",
    "    # MFCC\n",
    "    num_ceps = 12\n",
    "    mfcc = dct(filter_banks, type=2, axis=1, norm='ortho')[:, 1 : (num_ceps + 1)] # Keep 2-13\n",
    "    filter_banks -= (numpy.mean(filter_banks, axis=0) + 1e-8)\n",
    "    mfcc -= (numpy.mean(mfcc, axis=0) + 1e-8)\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
