{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "The second step in any speech recognition task is to use a classifier to classify the test sample based on the features extracted. The K Nearest Neighbour algorithm is one such supervised machine learing algorithm that can be used in classification problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Required libraries\n",
    "The required libraries are :\n",
    "<br>numpy\n",
    "<br>math\n",
    "<br>operator\n",
    "\n",
    "We first import the mentioned libraries, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import librosa\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Dynamic Time Warping\n",
    "Dynamic Time Warping is an algorithm used in time-series analysis for measuring similarity between sequences which vary in speed. A well-known application is in automatic speech recognition, to cope with different speaking speeds. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTW aims at aligning two sequences of feature vectors by warping the time axis iteratively until an optimal match (according to a suitable metrics) between the two sequences is found. \n",
    "<br>Consider two sequences of feature vectors:\n",
    "<br>A = a1,a2,a3....an\n",
    "<br>B = b1,b2,b3....bm\n",
    "The two sequences can be arranged in a grid as follows :\n",
    "<img src=\"images/dtw.png\">\n",
    "As it can be seen, both sequences start at the bottom left of the grid. \n",
    "Inside each cell a distance measure can be placed, comparing the corresponding elements of the two sequences. Our goal is to find a path through the grid that minimizes the total distance between them. Now for each step, we'll consider the distance between each points in concern and add it with the minimum distance we found so far. This will give us the optimal distance of two sequences up to that position. Hence, our formula will be,\n",
    "\n",
    "<br>Table[ i ][ j ] := d(i, j) + min(Table[ i-1 ][ j ], Table[ i-1 ][ j-1 ], Table[ i ][ j-1 ])\n",
    "<br>The overall distance will be the minimum of the sum of the distances between the individual elements. This is represented by the value in the last element of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtwDist(x, y, dist, warp=1, w=np.inf, s=1.0):\n",
    "    assert len(x)\n",
    "    assert len(y)\n",
    "    assert np.isinf(w) or (w >= abs(len(x) - len(y)))\n",
    "    assert s > 0\n",
    "    r, c = len(x), len(y)\n",
    "    if not np.isinf(w):\n",
    "        D0 = full((r + 1, c + 1), np.inf)\n",
    "        for i in range(1, r + 1):\n",
    "            D0[i, max(1, i - w):min(c + 1, i + w + 1)] = 0\n",
    "        D0[0, 0] = 0\n",
    "    else:\n",
    "        D0 = np.zeros((r + 1, c + 1))\n",
    "        D0[0, 1:] = np.inf\n",
    "        D0[1:, 0] = np.inf\n",
    "    D1 = D0[1:, 1:]  # view\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            if (np.isinf(w) or (max(0, i - w) <= j <= min(c, i + w))):\n",
    "                D1[i, j] = dist(x[i], y[j])\n",
    "    jrange = range(c)\n",
    "    for i in range(r):\n",
    "        if not np.isinf(w):\n",
    "            jrange = range(max(0, i - w), min(c, i + w + 1))\n",
    "        for j in jrange:\n",
    "            min_list = [D0[i, j]]\n",
    "            for k in range(1, warp + 1):\n",
    "                i_k = min(i + k, r)\n",
    "                j_k = min(j + k, c)\n",
    "                min_list += [D0[i_k, j] * s, D0[i, j_k] * s]\n",
    "            D1[i, j] += min(min_list)\n",
    "    return D1[-1,-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 K Nearest Neighbours\n",
    "Once the DTW distances are computed, the K nearest neighbour algorithm can be used to predict the class of the test data. \n",
    "<br> The first step is to compute the neighbours of the test instance. For this, we will be computing the DTW distance of the test instance from each of the train data. The K least distances will be selected and the corresponding neighbours will be computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNeighbours(mfcc_train_data,mfcc_test_instance,k,mfcc_train_labels):\n",
    "    distances=[]\n",
    "    neighbours=[]\n",
    "    \n",
    "    for i in range(len(mfcc_train_data)):\n",
    "        dist=dtwDist(mfcc_test_instance.T,mfcc_train_data[i].T,dist=lambda x, y: np.linalg.norm(x - y, ord=1))\n",
    "        distances.append((mfcc_train_labels[i],dist))\n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    for i in range(k):\n",
    "        neighbours.append(distances[i][0])\n",
    "    return neighbours\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class response is computed. The class response, in simple terms, computes the number of votes for each class of neighbours. The class with the highest votes represents the class response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassResponse(neighbours):\n",
    "    classvotes={}\n",
    "    for i in range(len(neighbours)):\n",
    "        response=neighbours[i][-1]\n",
    "        if response in classvotes:\n",
    "            classvotes[response]+=1\n",
    "        else:\n",
    "            classvotes[response]=1\n",
    "    sortedVotes = sorted(classvotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to determine how accurate your KNN is! The predictions are compared with the results of the test instances and accuracy is computed. Changing the parameter 'K' could change the accuracy in ways dependent on the nature of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(test_results,predictions):\n",
    "    correctPred = 0\n",
    "    for i in range(len(test_results)):\n",
    "        if test_results[i] == predictions[i]:\n",
    "            correctPred += 1\n",
    "    return round((correctPred/float(len(test_results))) * 100.0, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(mfcc_train_data,mfcc_test_data,k,test_results,mfcc_train_labels):\n",
    "    predictions=[]\n",
    "    for i in range(len(mfcc_test_data)):\n",
    "        neighbours=getNeighbours(mfcc_train_data,mfcc_test_data[i],k,mfcc_train_labels)\n",
    "        neighbour_pred=ClassResponse(neighbours)\n",
    "        predictions.append(neighbour_pred)\n",
    "    \n",
    "    accuracy=getAccuracy(test_results,predictions)\n",
    "    print('Accuracy = ', accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now test our code for the spoken-digit dataset. It contains around 2000 audio files, 200 each for each digit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = \"free-spoken-digit-dataset-master/recordings\"\n",
    "files = [f for f in os.listdir(dirname)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly sample 50 audio files of each class. This is done as it would take too much time to test our KNN on the entire dataset. We now test it for a limited number of audio files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_file=[]\n",
    "one_file=[]\n",
    "two_file=[]\n",
    "three_file=[]\n",
    "four_file=[]\n",
    "five_file=[]\n",
    "six_file=[]\n",
    "seven_file=[]\n",
    "eight_file=[]\n",
    "nine_file=[]\n",
    "for i in range(len(files)):\n",
    "    if(files[i][0]=='0'):\n",
    "        zero_file.append(files[i])\n",
    "    if(files[i][0]=='1'):\n",
    "        one_file.append(files[i])\n",
    "    if(files[i][0]=='2'):\n",
    "        two_file.append(files[i])\n",
    "    if(files[i][0]=='3'):\n",
    "        three_file.append(files[i])\n",
    "    if(files[i][0]=='4'):\n",
    "        four_file.append(files[i])\n",
    "    if(files[i][0]=='5'):\n",
    "        five_file.append(files[i])\n",
    "    if(files[i][0]=='6'):\n",
    "        six_file.append(files[i])\n",
    "    if(files[i][0]=='7'):\n",
    "        seven_file.append(files[i])\n",
    "    if(files[i][0]=='8'):\n",
    "        eight_file.append(files[i])\n",
    "    if(files[i][0]=='9'):\n",
    "        nine_file.append(files[i])\n",
    "files_sampled=random.sample(zero_file, 50)+random.sample(one_file, 50)+random.sample(two_file, 50)+random.sample(three_file, 50)+random.sample(four_file, 50)+random.sample(five_file, 50)+random.sample(six_file, 50)+random.sample(seven_file, 50)+random.sample(eight_file, 50)+random.sample(nine_file, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCC is extracted for all the audio files. The class labels are also extracted in the y_labels list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_list=[]\n",
    "y_labels=[]\n",
    "for i in range(len(files_sampled)):\n",
    "    y, sr = librosa.load(dirname+\"/\"+files[i])\n",
    "    mfcc1 = librosa.feature.mfcc(y,sr,n_mfcc=13)\n",
    "    mfcc_list.append(mfcc1)\n",
    "    y_labels.append(files[i][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split into train and test with the ratio 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "mfcc_train,mfcc_test,y_train,y_test=train_test_split(mfcc_list,y_labels,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  99.0 %\n"
     ]
    }
   ],
   "source": [
    "knn(mfcc_train,mfcc_test,5,y_test,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
